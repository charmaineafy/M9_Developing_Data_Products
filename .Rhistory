modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf,ntree=50)
train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=50)
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" # Training Dataset
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"  # Testing Dataset
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {dir.create("./data")}
if (!file.exists(trainFile)) {download.file(trainUrl, destfile=trainFile)}
if (!file.exists(testFile)) {download.file(testUrl, destfile=testFile)}
trainRaw <- read.csv(file=trainFile, header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
testRaw <- read.csv(file=trainFile, header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
# for creating predictive models
library(caret); library(rpart); library(randomForest); library(gbm)
# for multi-core and parallel processing
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
nzv_cols <- nearZeroVar(trainRaw)
if (length(nzv_cols) > 0) { trainClean1 <- trainRaw[,-nzv_cols] }
nzv_cols <- nearZeroVar(testRaw)
if (length(nzv_cols) > 0) { testClean1 <- testRaw[,-nzv_cols] }
cleanData <- function(df) {
idx.keep <- !sapply(df, function(x) any(is.na(x)))
df <- df[, idx.keep]
idx.keep <- !sapply(df, function(x) any(x==""))
df <- df[, idx.keep]
# Remove irrelevant predictor variables
col.rm <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window")
idx.rm <- which(colnames(df) %in% col.rm)
df <- df[, -idx.rm]
return(df)
}
trainClean2 <- cleanData(trainClean1)
testClean2 <- cleanData(testClean1)
set.seed(1234) # Consistency for reproducible results.
inTrain <- createDataPartition(trainClean2$classe, p=0.70, list=F)
trainData <- trainClean2[inTrain, ]
testData <- trainClean2[-inTrain, ]
ptm = proc.time()
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=50)
proc.time() - ptm
modelRf
predictRf <- predict(modelRf, testData)
confusionMatrix(testData$classe, predictRf)
accuracy <- postResample(predictRf, testData$classe)
print(paste("Model Accuracy: ", accuracy * 100, "%"))
accuracy <- postResample(predictRf, testData$classe)
print(paste("Model Accuracy: ", accuracy * 100, "%"))
accurary
accuracy <- postResample(predictRf, testData$classe)
accuracy
accuracy$Accuracy
accuracy$Accuracy
accuracy.Accuracy
accuracy
oose <- 1 - as.numeric(confusionMatrix(testData$classe, predictRf)$overall[1]) # out-of-sample error
oose
answers <- predict(modelRF, newdata=testing)
top testing
testing
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" # Training Dataset
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"  # Testing Dataset
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {dir.create("./data")}
if (!file.exists(trainFile)) {download.file(trainUrl, destfile=trainFile)}
if (!file.exists(testFile)) {download.file(testUrl, destfile=testFile)}
trainRaw <- read.csv(file=trainFile, header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
testRaw <- read.csv(file=trainFile, header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
# for creating predictive models
library(caret); library(rpart); library(randomForest); library(gbm)
# for multi-core and parallel processing
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
nzv_cols <- nearZeroVar(trainRaw)
if (length(nzv_cols) > 0) { trainClean1 <- trainRaw[,-nzv_cols] }
nzv_cols <- nearZeroVar(testRaw)
if (length(nzv_cols) > 0) { testClean1 <- testRaw[,-nzv_cols] }
cleanData <- function(df) {
idx.keep <- !sapply(df, function(x) any(is.na(x)))
df <- df[, idx.keep]
idx.keep <- !sapply(df, function(x) any(x==""))
df <- df[, idx.keep]
# Remove irrelevant predictor variables
col.rm <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window")
idx.rm <- which(colnames(df) %in% col.rm)
df <- df[, -idx.rm]
return(df)
}
trainClean2 <- cleanData(trainClean1)
testClean2 <- cleanData(testClean1)
testClean2$problem_id
testClean2
head(testClean2,1)
head(testClean2,100)
View(testRaw)
view(testRaw)
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" # Training Dataset
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"  # Testing Dataset
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {dir.create("./data")}
if (!file.exists(trainFile)) {download.file(trainUrl, destfile=trainFile)}
if (!file.exists(testFile)) {download.file(testUrl, destfile=testFile)}
trainRaw <- read.csv(file=trainFile, header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
testRaw <- read.csv(file=testFile, header=TRUE, sep=",", na.strings=c("NA","#DIV/0!",""))
summary(testRaw)
count(testRaw)
str(testRaw)
# for creating predictive models
library(caret); library(rpart); library(randomForest); library(gbm)
# for multi-core and parallel processing
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
nzv_cols <- nearZeroVar(trainRaw)
if (length(nzv_cols) > 0) { trainClean1 <- trainRaw[,-nzv_cols] }
nzv_cols <- nearZeroVar(testRaw)
if (length(nzv_cols) > 0) { testClean1 <- testRaw[,-nzv_cols] }
summary(testClean1)
str(testClean1)
cleanData <- function(df) {
idx.keep <- !sapply(df, function(x) any(is.na(x)))
df <- df[, idx.keep]
idx.keep <- !sapply(df, function(x) any(x==""))
df <- df[, idx.keep]
# Remove irrelevant predictor variables
col.rm <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
"new_window", "num_window")
idx.rm <- which(colnames(df) %in% col.rm)
df <- df[, -idx.rm]
return(df)
}
trainClean2 <- cleanData(trainClean1)
testClean2 <- cleanData(testClean1)
str(testClean2)
set.seed(1234) # Consistency for reproducible results.
inTrain <- createDataPartition(trainClean2$classe, p=0.70, list=F)
trainData <- trainClean2[inTrain, ]
testData <- trainClean2[-inTrain, ]
ptm = proc.time()
modelRF <- train(classe ~ ., data=trainData, method="rf", trControl=modelControl, ntree=100)
proc.time() - ptm
modelRF
modelControl <- trainControl(method="cv", 5)
ptm = proc.time()
modelRF <- train(classe ~ ., data=trainData, method="rf", trControl=modelControl, ntree=100)
proc.time() - ptm
modelRF
predictRF <- predict(modelRF, testData)
confusionMatrix(testData$classe, predictRF)
accuracy <- postResample(predictRF, testData$classe)
accuracy
oose <- 1 - as.numeric(confusionMatrix(testData$classe, predictRF)$overall[1]) # out-of-sample error
oose
answers <- predict(modelRF, newdata=testClean2)
str(answers)
answers
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
answers <- predict(modelRF, newdata=testClean2)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("./results/problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
if (!file.exists("./results")) {dir.create("./results")}
pml_write_files(answers)
rpart()
treeModel <- rpart(classe ~ ., data=trainData, method="class")
prp(treeModel)
library(rpart.plot)
install.packages(rpart.plot)
install.packages("rpart.plot")
prp(treeModel)
library(rpart.plot)
prp(treeModel)
setwd("D:/ANG/MOOC/M9-PROD/PROJECT/shinyapps")
setwd("D:/ANG/MOOC/M9-PROD/PROJECT")
getwd()
library("shiny")
library(shinyapps)
shinyapps::setAccountInfo(name='charmaineafy', token='D337B113F9C128B38E786796F8589CB8', secret='wEBCvK4XKdx+HUVsq9Hb11Q4AX4pZy8cR+ItsmGB')
deployApp()
deployApp()
runGitHub( “PROJECT”, “charmaineafy”)
library(slidify)
install_github('slidify','ramnathv')
library(devtools)
install_github('slidify','ramnathv')
install_github('slidify','ramnathv/slidify')
install_github('slidify','ramnathv)
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
library(slidify)
setwd("D:/ANG/MOOC/M9-PROD/PROJECT/M9_Developing_Data_Products")
getwd()
author("Titanic_App_Presentation")
getwd()
slidify('index.rmd')
library(knitr)
browseURL('index.html')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
publish(title = 'Titanic Survival Analysis App', 'index.html', host = 'rpubs')
getwd()
publish_rpubs(title = "Titanic Survival Analysis App", html_file = "index.html")
slidify('index.rmd')
browseURL('index.html')
slidify('index.rmd')
browseURL('index.html')
getwd()
publish_rpubs(title = "Titanic Survival Analysis App", html_file = "index.html")
shiny::runApp('D:/ANG/MOOC/M9-PROD/PROJECT/M9_Developing_Data_Products')
shiny::runApp('D:/ANG/MOOC/M9-PROD/PROJECT/M9_Developing_Data_Products')
library(shinyapps)
deployApp()
setwd("D:/ANG/MOOC/M9-PROD/PROJECT/M9_Developing_Data_Products")
deployApp()
shinyapps::setAccountInfo(name='charmaineafy', token='D337B113F9C128B38E786796F8589CB8', secret='wEBCvK4XKdx+HUVsq9Hb11Q4AX4pZy8cR+ItsmGB')
library(shinyapps)
getwd()
deployApp()
